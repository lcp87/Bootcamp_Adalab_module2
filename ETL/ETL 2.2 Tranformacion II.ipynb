{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear una clase con el código que usamos en los ejercicios de pair programming de ETL Transformación I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = {'usa': [39.7837304, -100.445882], \"australia\" : [-24.7761086, 134.755],\n",
    "        'south africa' : [-28.8166236, 24.991639], 'new zealand': [-41.5000831, 172.8344077], 'papua new guinea': [-5.6816069, 144.2489081]}\n",
    "\n",
    "\n",
    "lista_paises = ['usa','australia','new zealand','south africa','papua new guinea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extraer():\n",
    "    def __init__(self, diccionario):\n",
    "        self.diccionario = diccionario \n",
    "        self.paises = lista_paises\n",
    "    \n",
    "    def cargar_dfs():\n",
    "        df_attacks = pd.read_csv(\"attacks_limpieza_completa.csv\", index_col = 0)\n",
    "        lista_paises = ['usa','australia','new zealand','south africa','papua new guinea']\n",
    "        df_paises = df_attacks[df_attacks['country'].isin(lista_paises)]\n",
    "        df_clima = pd.read_csv(\"datos_clima.csv\", index_col = 0)\n",
    "\n",
    "      \n",
    "    def llamar_API(diccionario):\n",
    "        \n",
    "    #Hacemos un for para ir iterando por los paises y sus valores e ir extrayendo la info de la API\n",
    "        for key, value in diccionario.items():\n",
    "            producto = 'meteo'\n",
    "            lat = value[0]\n",
    "            lon = value[1]\n",
    "            url = f'http://www.7timer.info/bin/api.pl?lon=-{lon}&lat={lat}&product={producto}&output=json'\n",
    "            response = requests.get(url=url)\n",
    "            response.status_code\n",
    "            response.reason\n",
    "            df = pd.DataFrame.from_dict(pd.json_normalize(response.json()['dataseries']))\n",
    "            #Creamos una columna con el nombre del pais\n",
    "            df['pais'] = key\n",
    "            return df\n",
    "\n",
    "    # esto está repetido arriba...\n",
    "    # aquí estaba cargar_dfs() repetido...\n",
    "\n",
    "    #Concatenamos los archivos, creamos un df vacio para ir rellenandolo con la info que queremos\n",
    "    def juntar_dfs(self, df_completo, df_civil, df_visibilidad, df_meteo): \n",
    "    \n",
    "        # lo primero que hacemos es concatenar los dataframes con la información general. df_completo y df_civil\n",
    "        df_completo = pd.concat([df_completo, df_civil], axis = 0)\n",
    "\n",
    "        # ahora es el turno de unir el dataframe con la información de la visibilidad con el completo\n",
    "        # en este caso el how lo ponemos como left ya que queremos que se quede con toda la info que en la primera tabla que le pasamos que es la que tiene toda la información\n",
    "        df_completo = pd.merge(df_completo , df_visibilidad , on=['fecha', \"timepoint\"], how = \"inner\")\n",
    "        df_completo = pd.merge(df_completo, df_meteo, on=['fecha'], how = \"left\")\n",
    "        # guardamos los datos\n",
    "        df_completo.to_pickle('./datos_actualizados.pkl')\n",
    "        df_completo.to_csv('./datos_actualizados.csv')\n",
    "\n",
    "        return df_completo\n",
    "\n",
    "    def limpiar_dataframe(self, df, lista_columnas): \n",
    "\n",
    "       #convertimos la fecha a datetime\n",
    "        df[\"fecha\"] = pd.to_datetime(df[\"fecha\"])\n",
    "\n",
    "        # reemplazamos los nulos de las columnas por la media\n",
    "         # lista de columnas en las que queremos reemplazar los nulos\n",
    "        df[lista_columnas]=df[lista_columnas].fillna(df.mean())\n",
    "\n",
    "        # quitar columnas repetidas:\n",
    "        \n",
    "        df.drop([\"ciudad_y\"], axis = 1, inplace = True)\n",
    "        \n",
    "        # renombrar columnas\n",
    "        \n",
    "        df.rename(columns = {\"ciudad_x\": \"ciudad\"}, inplace = True)\n",
    "\n",
    "        # quitar % \n",
    "        df[\"rh2m\"] = df[\"rh2m\"].replace(r\"%\", \"\", regex = True)\n",
    "\n",
    "        # guardamos los datos una vez limpios\n",
    "        df.to_pickle('./datos_Madrid.pkl')\n",
    "        df.to_csv('./datos_Madrid.csv')\n",
    "\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2a25da4702e7ca1aacc8a7d4e81059851e19fd1060402175b35fae4c2a518e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
